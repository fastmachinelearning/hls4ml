=================
VivadoAccelerator
=================

The **VivadoAccelerator** backend of ``hls4ml`` leverages the `PYNQ <http://pynq.io/>`_ software stack to easily deploy models on supported devices.
Currently ``hls4ml`` supports the following boards:

* `pynq-z2 <https://www.xilinx.com/support/university/xup-boards/XUPPYNQ-Z2.html>`_ (part: ``xc7z020clg400-1``)
* `zcu102 <https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html>`_ (part: ``xczu9eg-ffvb1156-2-e``)
* `alveo-u50 <https://www.xilinx.com/products/boards-and-kits/alveo/u50.html>`_ (part: ``xcu50-fsvh2104-2-e``)
* `alveo-u250 <https://www.xilinx.com/products/boards-and-kits/alveo/u250.html>`_ (part: ``xcu250-figd2104-2L-e``)
* `alveo-u200 <https://www.xilinx.com/products/boards-and-kits/alveo/u200.html>`_ (part: ``xcu200-fsgd2104-2-e``)
* `alveo-u280 <https://www.xilinx.com/products/boards-and-kits/alveo/u280.html>`_ (part: ``xcu280-fsvh2892-2L-e``)

but, in principle, support can be extended to `any board supported by PYNQ <http://www.pynq.io/board.html>`_.
For the Zynq-based boards, there are two components: an ARM-based processing system (PS) and FPGA-based programmable logic (PL), with various interfaces between the two.

.. image:: ../img/zynq_interfaces.png
  :height: 300px
  :align: center
  :alt: Zynq PL/PS interfaces

Neural Network Overlay
======================

In the PYNQ project, programmable logic circuits are presented as hardware libraries called *overlays*.
The overlay can be accessed through a Python API.
In ``hls4ml``, we create a custom **neural network overlay**, which sends and receives data via AXI stream.
The target device is programmed using a bitfile that is generated by the ``VivadoAccelerator`` backend.

.. image:: ../img/pynqframe.png
  :width: 600px
  :align: center
  :alt: PYNQ software stack

Example
=======

This example is taken from `part 7 of the hls4ml tutorial <https://github.com/fastmachinelearning/hls4ml-tutorial/blob/master/part7_deployment.ipynb>`_.
Specifically, we'll deploy a model on a ``pynq-z2`` board.

First, we generate the bitfile from a Keras model ``model`` and a config.

.. code-block:: Python

    import hls4ml
    config = hls4ml.utils.config_from_keras_model(model, granularity='name')
    hls_model = hls4ml.converters.convert_from_keras_model(model,
                                                           hls_config=config,
                                                           output_dir='hls4ml_prj_pynq',
                                                           backend='VivadoAccelerator',
                                                           board='pynq-z2')
    hls4ml.build(bitfile=True)


After this command completes, we will need to package up the bitfile, hardware handoff, and Python driver to copy to the PS of the board.

.. code-block:: bash

    mkdir -p package
    cp hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.runs/impl_1/design_1_wrapper.bit package/hls4ml_nn.bit
    cp hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hw_handoff/design_1.hwh package/hls4ml_nn.hwh
    cp hls4ml_prj_pynq/axi_stream_driver.py package/
    tar -czvf package.tar.gz -C package/ .

Then we can copy this package to the PS of the board and untar it.

Finally, on the PS in Python we can create a ``NeuralNetworkOverlay`` object, which will download the bitfile onto the PL of the board.
We also must provide the shapes of our input and output data, ``X_test.shape`` and ``y_test.shape``, respectively, to allocate the buffers for the data transfer.
The ``predict`` method will send the input data to the PL and return the output data ``y_hw``.

.. code-block:: Python

    from axi_stream_driver import NeuralNetworkOverlay

    nn = NeuralNetworkOverlay('hls4ml_nn.bit', X_test.shape, y_test.shape)
    y_hw, latency, throughput = nn.predict(X_test, profile=True)

================
VitisAccelerator
================

The ``VitsAccelerator`` backend leverages the `Vitis System Design Flow <https://www.xilinx.com/products/design-tools/vitis.html#design-flows>`_ to automate and simplify the creation of an hls4ml project targeting `AMD Alveo PCIe accelerators <https://www.amd.com/en/products/accelerators/alveo.html>`_.
The Vitis accelerator backend has been tested with the following boards:

* `Alveo u50 <https://www.xilinx.com/products/boards-and-kits/alveo/u50.html>`_
* `Alveo u55c <https://www.xilinx.com/products/boards-and-kits/alveo/u55c.html>`_
* `Alveo u250 <https://www.xilinx.com/products/boards-and-kits/alveo/u250.html>`_
* `Versal vck5000 <https://www.xilinx.com/products/boards-and-kits/vck5000.html>`_

Kernel wrapper
==============

To integrate with the Vitis System Design Flow and run on an accelerator, the generated ``hls4ml`` model must be encapsulated and built as a Vitis kernel (``.xo`` file) and linked into a binary file (``.xclbin``) during the implementation step. On the host side, standard C++ code using either `OpenCL <https://xilinx.github.io/XRT/master/html/opencl_extension.html>`_ or `XRT API <https://xilinx.github.io/XRT/master/html/xrt_native_apis.html>`_ can be used to download the ``.xclbin`` file to the accelerator card and use any kernel it contains.

The ``VitisAccelerator`` backend automatically generates a kernel wrapper, an host code example, and a Makefile to build the project.

**Note:** The current implementation of the kernel wrapper code is oriented toward throughput benchmarking and not general inference uses (See :ref:`here<hardware_predict-method>`). It can nonetheless be further customized to fit specific applications.

Options
=======

As PCIe accelerators are not suitable for ultra-low latency applications, it is assumed that they are used for high-throughput applications. To accommodate this, the backend supports the following options to optimize the kernel for throughput:

    * ``num_kernel``: Number of kernel instance to implement in the hardware architecture.
    * ``num_thread``: Number of host threads used to exercise the kernels in the host application.
    * ``batchsize``: Number of samples to be processed in a single kernel execution.

Additionaly, the backend proposes the following options to customize the implementation:

    * ``board``: The target board, must match one entry in ``supported_boards.json``.
    * ``clock_period``: The target clock period in ns.
    * ``hw_quant``: Is arbitrary precision quantization performed in hardware or not. If True, the quantization is performed in hardware and float are used at the kernel interface, otherwise it is performed in software and arbitrary precision types are used at the interface. (Defaults to  ``False``).
    * ``vivado_directives``: A list of strings to be added under the ``[Vivado]`` section of the generated ``accelerator_card.cfg`` link configuration file. Can be used to add custom directives to the Vivado project.

Build workflow
==============

At the call of the ``build`` method, the following option affect the build process:

    * ``reset``: If True, clears files generated during previous build processes (Equivalent to ``make clean`` in build folder).
    * ``target``: Can be one of ``hw``, ``hw_emu``, ``sw_emu``, to define which build target to use (Default is ``hw``).
    * ``debug``: If True, compiles the c++ host code and the HLS in debug mode.

Once the project is generated, it possible to run manually the build steps by using one of the following ``make`` targets in the generated project directory:

    * ``host``: Compiles the host application.
    * ``hls``: Produces only the kernel's object file.
    * ``xclbin``: Produces only the kernel's .xclbin file.
    * ``clean``: Removes all generated files.
    * ``run``: Run the host application using the .xclbin file and the input data present in ``tb_data/tb_input_features.dat``.

It is also possible to run the full build process by calling ``make`` without any target. Modifications to the ``accelerator_card.cfg`` file can be done manually before running the build process (e.g., to change the clock period, or add addition ``.xo`` kernel to the build).

Host code
=========

Once built, the host program can be run to load the board and perform inferences:

.. code-block:: Bash

    ./host

By defaut, all Computing Unit (CU) on all compatible devices will be used, with 3 worker thread per CU.

The generated host code application support the following options to tweak the execution:

 * ``-d``: device BDF to use (can be specified multiple times)
 * ``-x``: XCLBIN path
 * ``-i``: input feature file
 * ``-o``: output feature file
 * ``-c``: maximum computing units count to use
 * ``-n``: number of worker threads to use
 * ``-r``: number of repeatition of the input feature file (For artificially increasing the data size for benchmarking purpose)
 * ``-v``: enable verbose output
 * ``-h``: print help

The following example shows how to limit on only one device, one CU, and on worker thread:

.. code-block:: Bash

    ./host -d 0000:c1:00.1 -c 1 -n 1

Example
=======

The following example is a modified version of `hsl4ml example 7 <https://github.com/fastmachinelearning/hls4ml-tutorial/blob/master/part7_deployment.ipynb>`_.

.. code-block:: Python

    import hls4ml
    hls_model = hls4ml.converters.convert_from_keras_model(
        model,
        hls_config=config,
        output_dir='model_3/hls4ml_prj_vitis_accel',
        backend='VitisAccelerator',
        board='alveo-u55c',
        num_kernel=4,
        num_thread=8,
        batchsize=8192,
        hw_quant=False,
        vivado_directives=["prop=run.impl_1.STEPS.PLACE_DESIGN.ARGS.DIRECTIVE=Explore"]
    )
    hls_model.compile()
    hls_model.build()
    y = hls_model.predict_hardware(y) # Limited to batchsize * num_kernel * num_thread for now
