<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>hls4ml.optimization.keras package &mdash; hls4ml 0.8.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_contributors.css" />

  
    <link rel="shortcut icon" href="../_static/hls4ml_logo.svg"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="hls4ml.optimization.objectives package" href="hls4ml.optimization.objectives.html" />
    <link rel="prev" title="hls4ml.optimization package" href="hls4ml.optimization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../index.html">
            
              <img src="../_static/hls4ml_logo_navbar.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.9.0.dev3+g33f8ffbe
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../status.html">Status and Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../setup.html">Setup and Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../details.html">Software Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../flows.html">Optimizer Passes and Flows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../command.html">Command Line Interface (deprecated)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference.html">Citation, Acknowledgments, and Contributors</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/hls-model.html">HLS Model Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/profiling.html">Profiling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/fifo_depth.html">FIFO Buffer Depth Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/extension.html">Extension API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/accelerator.html">VivadoAccelerator Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/model_optimization.html">Hardware-aware Optimization API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Autogenerated API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hls4ml.backends.html">hls4ml.backends package</a></li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.converters.html">hls4ml.converters package</a></li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.model.html">hls4ml.model package</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="hls4ml.optimization.html">hls4ml.optimization package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="hls4ml.optimization.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.html#module-hls4ml.optimization.attributes">hls4ml.optimization.attributes module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.html#module-hls4ml.optimization.config">hls4ml.optimization.config module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.html#module-hls4ml.optimization.knapsack">hls4ml.optimization.knapsack module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.html#module-hls4ml.optimization.scheduler">hls4ml.optimization.scheduler module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls4ml.optimization.html#module-hls4ml.optimization">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.report.html">hls4ml.report package</a></li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.utils.html">hls4ml.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="hls4ml.writer.html">hls4ml.writer package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">hls4ml</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="hls4ml.optimization.html">hls4ml.optimization package</a></li>
      <li class="breadcrumb-item active">hls4ml.optimization.keras package</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/fastmachinelearning/hls4ml/blob/main/docs/autodoc/hls4ml.optimization.keras.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="hls4ml-optimization-keras-package">
<h1>hls4ml.optimization.keras package<a class="headerlink" href="#hls4ml-optimization-keras-package" title="Permalink to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading"></a></h2>
</section>
<section id="module-hls4ml.optimization.keras.builder">
<span id="hls4ml-optimization-keras-builder-module"></span><h2>hls4ml.optimization.keras.builder module<a class="headerlink" href="#module-hls4ml.optimization.keras.builder" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.builder.HyperOptimizationModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.builder.</span></span><span class="sig-name descname"><span class="pre">HyperOptimizationModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attributes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_metric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regularization_range</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.builder.HyperOptimizationModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">HyperModel</span></code></p>
<p>Helper class for Keras Tuner</p>
<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.builder.HyperOptimizationModel.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hp</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.builder.HyperOptimizationModel.build" title="Permalink to this definition"></a></dt>
<dd><p>Builds a model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hp</strong> – A <cite>HyperParameters</cite> instance.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A model instance.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.builder.build_optimizable_model">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.builder.</span></span><span class="sig-name descname"><span class="pre">build_optimizable_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attributes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_metric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">increasing</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">directory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'hls4ml-optimization-keras'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tuner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Bayesian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regularization_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[1e-06,</span> <span class="pre">1.8478497974222906e-06,</span> <span class="pre">3.414548873833601e-06,</span> <span class="pre">6.30957344480193e-06,</span> <span class="pre">1.165914401179831e-05,</span> <span class="pre">2.1544346900318823e-05,</span> <span class="pre">3.9810717055349695e-05,</span> <span class="pre">7.356422544596421e-05,</span> <span class="pre">0.00013593563908785255,</span> <span class="pre">0.00025118864315095795,</span> <span class="pre">0.00046415888336127773,</span> <span class="pre">0.0008576958985908938,</span> <span class="pre">0.001584893192461114,</span> <span class="pre">0.0029286445646252374,</span> <span class="pre">0.0054116952654646375,</span> <span class="pre">0.01]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.builder.build_optimizable_model" title="Permalink to this definition"></a></dt>
<dd><p>Function identifying optimizable layers and adding a regularization loss</p>
<p>Notes:
- In general, the regularization and learning rate ranges do not need to be provided,
as the implementation sets a generic enough range. if the user has an idea on the
possible range on hyperparameter ranges, the tuning will complete faster.
- The default tuner is Bayesian &amp; when coupled with the correct ranges of hyperparameters,
it performs quite well, fast. However, older version of Keras Tuner had a crashing bug with it.
- In general, the directory does not need to be specified. However, if pruning several models simultaneously,
to avoid conflicting intermediate results, it is useful to specify directory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – Model to be optimized</p></li>
<li><p><strong>attributes</strong> (<em>dict</em>) – Layer-wise model attributes, obtained from hls4ml.optimization.get_attributes_from_keras_model()</p></li>
<li><p><strong>optimizer</strong> (<em>keras.optimizers.Optimizer</em>) – Optimizer used during training</p></li>
<li><p><strong>loss_fn</strong> (<em>keras.losses.Loss</em>) – Loss function used during training</p></li>
<li><p><strong>validation_metric</strong> (<em>keras.metrics.Metric</em>) – Validation metric, used as a baseline</p></li>
<li><p><strong>train_dataset</strong> (<em>tf.Dataset</em>) – Training inputs and labels, in the form of an iterable TF Dataset</p></li>
<li><p><strong>validation_dataset</strong> (<em>tf.Dataset</em>) – Validation inputs and labels, in the form of an iterable TF Dataset</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size during training</p></li>
<li><p><strong>epochs</strong> (<em>int</em>) – Maximum number of epochs to fine-tune model, in one iteration of pruning</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – Whether to log tuner outputs to the console</p></li>
<li><p><strong>directory</strong> (<em>string</em>) – Directory to store tuning results</p></li>
<li><p><strong>tuner</strong> (<em>str</em>) – Tuning algorithm, choose between Bayesian and Hyperband</p></li>
<li><p><strong>regularization_range</strong> (<em>list</em>) – List of suitable hyperparameters for weight decay</p></li>
<li><p><strong>learning_rate_range</strong> (<em>list</em>) – List of suitable hyperparameters for learning rate</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Model prepared for optimization</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.builder.remove_custom_regularizers">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.builder.</span></span><span class="sig-name descname"><span class="pre">remove_custom_regularizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.builder.remove_custom_regularizers" title="Permalink to this definition"></a></dt>
<dd><p>Helper function to remove custom regularizers (DenseRegularizer &amp; Conv2DRegularizer)
This makes it possible to load the model in a different environment without hls4ml installed</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>keras.Model</em>) – Baseline model</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Model without custom regularizers</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-hls4ml.optimization.keras.config">
<span id="hls4ml-optimization-keras-config-module"></span><h2>hls4ml.optimization.keras.config module<a class="headerlink" href="#module-hls4ml.optimization.keras.config" title="Permalink to this heading"></a></h2>
<dl class="py data">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.config.SUPPORTED_LAYERS">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.config.</span></span><span class="sig-name descname"><span class="pre">SUPPORTED_LAYERS</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">(&lt;class</span> <span class="pre">'keras.src.layers.core.dense.Dense'&gt;,</span> <span class="pre">&lt;class</span> <span class="pre">'keras.src.layers.convolutional.conv2d.Conv2D'&gt;,</span> <span class="pre">&lt;class</span> <span class="pre">'qkeras.qlayers.QDense'&gt;,</span> <span class="pre">&lt;class</span> <span class="pre">'qkeras.qconvolutional.QConv2D'&gt;)</span></em><a class="headerlink" href="#hls4ml.optimization.keras.config.SUPPORTED_LAYERS" title="Permalink to this definition"></a></dt>
<dd><p>Supported ranking metrics, for classifying redundant (groups of) weights</p>
<ol class="arabic simple">
<li><p>l1 - groups of weights are ranked by their l1 norm</p></li>
<li><p>l2 - groups of weights are ranked by their l2 norm</p></li>
<li><dl class="simple">
<dt>oracle - abs(dL / dw * w), introduced by Molchanov et al. (2016)</dt><dd><p>Pruning Convolutional Neural Networks for Resource Efficient Inference</p>
</dd>
</dl>
</li>
<li><p>saliency - (d^2L / dw^2 * w)^2, introduced by Lecun et al. (1989) Optimal Brain Damage</p></li>
</ol>
</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.config.SUPPORTED_METRICS">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.config.</span></span><span class="sig-name descname"><span class="pre">SUPPORTED_METRICS</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">('l1',</span> <span class="pre">'l2',</span> <span class="pre">'oracle',</span> <span class="pre">'saliency')</span></em><a class="headerlink" href="#hls4ml.optimization.keras.config.SUPPORTED_METRICS" title="Permalink to this definition"></a></dt>
<dd><p>Temporary directory for storing best models, tuning results etc.</p>
</dd></dl>

</section>
<section id="module-hls4ml.optimization.keras.masking">
<span id="hls4ml-optimization-keras-masking-module"></span><h2>hls4ml.optimization.keras.masking module<a class="headerlink" href="#module-hls4ml.optimization.keras.masking" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.masking.get_model_masks">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.masking.</span></span><span class="sig-name descname"><span class="pre">get_model_masks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">keras_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_attributes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hessians</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">knapsack_solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CBC_MIP'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.masking.get_model_masks" title="Permalink to this definition"></a></dt>
<dd><p>Function calculating a binary mask for all optimizable layers
Entries equal to one correspond to the weight being updated during the training
Entries equal to zero correspond to the weight being frozen during the training</p>
<dl class="simple">
<dt>Masking is such that:</dt><dd><ul class="simple">
<li><p>resource_utilization &lt;= (1 - sparsity) * baseline_utilization OR</p></li>
<li><p>resource_saving &gt; sparsity * baseline_utilization [equivalent formulation]</p></li>
</ul>
</dd>
</dl>
<p>Offsets are used for weight sharing - in the case of weight sharing, the mask is set to zero
Therefore, the weights will be frozen during training; however, they still need to be the mean of the group
Offsets represent the mean of each weight-shared group - therefore, it is important to have offsets only for
frozen weights; that is where the corresponding entry in the mask tensor is zero</p>
<p>If a layer supports both weight sharing and pruning, both the norm and variance of the group are calculated
And the smaller one is considered; so if the norm is smaller, the group will be considered for pruning
Otherise, the group will be considered for weight sharing.
Both the norm and variance are normalized, to avoid magnitude biases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keras_model</strong> (<em>keras.model</em>) – Model to be masked</p></li>
<li><p><strong>model_attributes</strong> (<em>dict</em>) – A layer-wise dictionary of LayerAttributes classes</p></li>
<li><p><strong>sparsity</strong> (<em>float</em>) – Desired sparsity, with respect to the objective</p></li>
<li><p><strong>objective</strong> (<a class="reference internal" href="hls4ml.optimization.objectives.html#hls4ml.optimization.objectives.ObjectiveEstimator" title="hls4ml.optimization.objectives.ObjectiveEstimator"><em>ObjectiveEstimator</em></a>) – Objective to be minimized (e.g. DSP, FLOPs etc.)</p></li>
<li><p><strong>metric</strong> (<em>string</em>) – Weight ranking metric - l1, l2, Oracle, saliency</p></li>
<li><p><strong>local</strong> (<em>boolean</em>) – Equal layer-wise sparsity</p></li>
<li><p><strong>gradients</strong> (<em>dict</em>) – A layer-wise dictionary of weight gradients
(needed for Oracle ranking)</p></li>
<li><p><strong>hessians</strong> (<em>dict</em>) – A layer-wise dictionary of second gradients
(needed for saliency ranking)</p></li>
<li><p><strong>knapsack_solver</strong> (<em>str</em>) – Algorithm for solving Knapsack problem; recommended is to use default.
Unless dealing with highly dimensional problems, in which case greedy is better.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>tuple containing</p>
<ul class="simple">
<li><p>masks (dict): Layer-wise dictionary of binary tensors</p></li>
<li><p>offsets (dict): Layer-wise dictionary of offsets for every weight</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-hls4ml.optimization.keras.reduction">
<span id="hls4ml-optimization-keras-reduction-module"></span><h2>hls4ml.optimization.keras.reduction module<a class="headerlink" href="#module-hls4ml.optimization.keras.reduction" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.reduction.reduce_model">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.reduction.</span></span><span class="sig-name descname"><span class="pre">reduce_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.reduction.reduce_model" title="Permalink to this definition"></a></dt>
<dd><p>Function for removing zero neurons &amp; filters from a model and rewiring the model graph
This function is built on top of Keras Surgeon available at: <a class="reference external" href="https://github.com/BenWhetton/keras-surgeon">https://github.com/BenWhetton/keras-surgeon</a>
Keras Surgeon is no longer under active development and does not work for TensorFlow 2.3+ and QKeras
The baseline version was forked and updated, available at: <a class="reference external" href="https://github.com/fastmachinelearning/keras-surgeon">https://github.com/fastmachinelearning/keras-surgeon</a></p>
<p>IMPORTANT: To use this funcionality please install separately from the above GitHub.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>keras.model</em>) – Input model</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Modified model, with redundant structures removed</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>reduced (keras.model)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-hls4ml.optimization.keras.regularizers">
<span id="hls4ml-optimization-keras-regularizers-module"></span><h2>hls4ml.optimization.keras.regularizers module<a class="headerlink" href="#module-hls4ml.optimization.keras.regularizers" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.regularizers.Conv2DRegularizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.regularizers.</span></span><span class="sig-name descname"><span class="pre">Conv2DRegularizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">structure_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">SUPPORTED_STRUCTURES.UNSTRUCTURED</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pattern_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">consecutive_patterns</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.regularizers.Conv2DRegularizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Regularizer</span></code></p>
<p>A flexible regularizer for Conv2D layers, simultaneously performing pruning and clustering</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em>) – Sparse penalty; a higher value pushes more weights towards zero</p></li>
<li><p><strong>beta</strong> (<em>float</em>) – Variance penalty; a higher value reduces variance between a group of weights</p></li>
<li><p><strong>norm</strong> (<em>int</em>) – Norm type (l1 or l2)</p></li>
<li><p><strong>structure_type</strong> (<em>string</em>) – Type of regularization - unstructured, structured, pattern</p></li>
<li><p><strong>pattern_offset</strong> (<em>int</em>) – Length of each pattern if structure_type == pattern</p></li>
<li><p><strong>weights</strong> (<em>tf.Variable</em>) – Four-dimensional layer weight tensor, dimensionality
(filter_width x filter_height x n_chan x n_filt)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Penalty associated with layer weights</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Regularizer penalty (tf.Variable)</p>
</dd>
</dl>
<dl class="simple">
<dt>Example use cases:</dt><dd><ul class="simple">
<li><p>structure_type = unstructured: unstructured weight regularization</p></li>
<li><dl class="simple">
<dt>structure_type = structured: filter regularization</dt><dd><p>(group weights of dimensionality filt_width x filt_height x n_chan)</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>structure_type = pattern: regularization on groups of every n-th weight in flattened array</dt><dd><p>(e.g. grouping by reuse factor in hls4ml)</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.regularizers.Conv2DRegularizer.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.regularizers.Conv2DRegularizer.get_config" title="Permalink to this definition"></a></dt>
<dd><p>Returns the config of the regularizer.</p>
<p>An regularizer config is a Python dictionary (serializable)
containing all configuration parameters of the regularizer.
The same regularizer can be reinstantiated later
(without any saved state) from this configuration.</p>
<p>This method is optional if you are just training and executing models,
exporting to and from SavedModels, or using weight checkpoints.</p>
<p>This method is required for Keras <cite>model_to_estimator</cite>, saving and
loading models to HDF5 formats, Keras model cloning, some visualization
utilities, and exporting models to and from JSON.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.regularizers.DenseRegularizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.regularizers.</span></span><span class="sig-name descname"><span class="pre">DenseRegularizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">structure_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">SUPPORTED_STRUCTURES.UNSTRUCTURED</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(1,</span> <span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pattern_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">consecutive_patterns</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.regularizers.DenseRegularizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Regularizer</span></code></p>
<p>A flexible regularizer for Dense layers, simultaneously penalizing high values and variance</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em>) – Sparse penalty; a higher value pushes more weights towards zero</p></li>
<li><p><strong>beta</strong> (<em>float</em>) – Variance penalty; a higher value reduces variance between a group of weights</p></li>
<li><p><strong>norm</strong> (<em>int</em>) – Norm type (l1 or l2)</p></li>
<li><p><strong>structure_type</strong> (<em>string</em>) – Type of regularization - unstructured, structured, pattern, block</p></li>
<li><p><strong>block_shape</strong> (<em>tuple</em>) – Block shape if structure_type == block</p></li>
<li><p><strong>pattern_offset</strong> (<em>int</em>) – Length of each pattern if structure_type == pattern</p></li>
<li><p><strong>consecutive_patterns</strong> (<em>int</em>) – How many consecutive patterns should be considered</p></li>
<li><p><strong>weights</strong> (<em>tf.Variable</em>) – Two-dimensional layer weight tensor, dimensionality (M x N)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Penalty associated with layer weights</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Regularizer penalty (tf.Variable)</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<ul class="simple">
<li><p>structure_type = unstructured: unstructured weight regularization</p></li>
<li><dl class="simple">
<dt>structure_type = structured: neuron regularization</dt><dd><p>(group weights by row)</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>structure_type = pattern: regularization on groups of every n-th weight</dt><dd><p>(e.g. grouping by reuse factor in hls4ml)</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>structure_type = block: regularization on blocks within weight matrix</dt><dd><p>(e.g. 4x4, 8x1 for certain SIMD processors)</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>consecutive_patterns is commonly encountered with optimization of BRAM utilization -</dt><dd><p>e.g. while it is true that each DSP pattern consumes one DSP,
They likely use less than one BRAM block (e.g. if the BRAM width is 36 bit and weight width is 16)
In that case, we need to group several patterns together,
So the entire block of patterns can be removed, thus saving DSP and BRAM</p>
</dd>
</dl>
</li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.regularizers.DenseRegularizer.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.regularizers.DenseRegularizer.get_config" title="Permalink to this definition"></a></dt>
<dd><p>Returns the config of the regularizer.</p>
<p>An regularizer config is a Python dictionary (serializable)
containing all configuration parameters of the regularizer.
The same regularizer can be reinstantiated later
(without any saved state) from this configuration.</p>
<p>This method is optional if you are just training and executing models,
exporting to and from SavedModels, or using weight checkpoints.</p>
<p>This method is required for Keras <cite>model_to_estimator</cite>, saving and
loading models to HDF5 formats, Keras model cloning, some visualization
utilities, and exporting models to and from JSON.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-hls4ml.optimization.keras.utils">
<span id="hls4ml-optimization-keras-utils-module"></span><h2>hls4ml.optimization.keras.utils module<a class="headerlink" href="#module-hls4ml.optimization.keras.utils" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.utils.get_last_layer_with_weights">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.utils.</span></span><span class="sig-name descname"><span class="pre">get_last_layer_with_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.utils.get_last_layer_with_weights" title="Permalink to this definition"></a></dt>
<dd><p>Finds the last layer with weights</p>
<p>The last layer with weights determined the output shape, so, pruning is sometimes not applicable to it.
As an example, consider a network with 16 - 32 - 5 neurons - the last layer’s neuron (5) cannot be removed
since they map to the data labels</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>keras.model</em>) – Input model</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Index location of last layer with params</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>idx (int)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.utils.get_model_gradients">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.utils.</span></span><span class="sig-name descname"><span class="pre">get_model_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.utils.get_model_gradients" title="Permalink to this definition"></a></dt>
<dd><p>Calculate model gradients with respect to weights</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.model</em>) – Input model</p></li>
<li><p><strong>loss_fn</strong> (<em>keras.losses.Loss</em>) – Model loss function</p></li>
<li><p><strong>X</strong> (<em>np.array</em>) – Input data</p></li>
<li><p><strong>y</strong> (<em>np.array</em>) – Output data</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Per-layer gradients of loss with respect to weights</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>grads (dict)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.utils.get_model_hessians">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.utils.</span></span><span class="sig-name descname"><span class="pre">get_model_hessians</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.utils.get_model_hessians" title="Permalink to this definition"></a></dt>
<dd><p>Calculate the second derivatives of the loss with repsect to model weights.</p>
<p>Note, only diagonal elements of the Hessian are computed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.model</em>) – Input model</p></li>
<li><p><strong>loss_fn</strong> (<em>keras.losses.Loss</em>) – Model loss function</p></li>
<li><p><strong>X</strong> (<em>np.array</em>) – Input data</p></li>
<li><p><strong>y</strong> (<em>np.array</em>) – Output data</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Per-layer second derivatives of loss with respect to weights</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>grads (dict)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.utils.get_model_sparsity">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.utils.</span></span><span class="sig-name descname"><span class="pre">get_model_sparsity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.utils.get_model_sparsity" title="Permalink to this definition"></a></dt>
<dd><p>Calculate total and per-layer model sparsity</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>-</em>) – Model to be evaluated</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>tuple containing</p>
<ul class="simple">
<li><p>sparsity (float): Model sparsity, as a percentage of zero weights w.r.t to total number of model weights</p></li>
<li><p>layers (dict): Key-value dictionary; each key is a layer name and the associated value is the layer’s sparsity</p></li>
</ul>
</p>
</dd>
</dl>
<p>TODO - Extend support for recurrent layers (reccurent_kernel)</p>
</dd></dl>

</section>
<section id="module-hls4ml.optimization.keras">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-hls4ml.optimization.keras" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.MaskedBackprop">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.</span></span><span class="sig-name descname"><span class="pre">MaskedBackprop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.MaskedBackprop" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A helper class to perform masked backprop (training with frozen weights)
The important function is __call__ as it masks gradients, based on frozen weights
While this function can exist without a class, taking masks as input would deplete memory
Since a new graph is created for every call, causing a large run-time
The trick is to set the masks, models etc. as class variables and then pass the sparsity
As the sparsity changes, a new graph of the function is created</p>
<dl class="py method">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.MaskedBackprop.update_masks">
<span class="sig-name descname"><span class="pre">update_masks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">masks</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.MaskedBackprop.update_masks" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="hls4ml.optimization.keras.optimize_model">
<span class="sig-prename descclassname"><span class="pre">hls4ml.optimization.keras.</span></span><span class="sig-name descname"><span class="pre">optimize_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_attributes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_val</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_val</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_metric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">increasing</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rtol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranking_metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rewinding_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff_bad_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">directory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'hls4ml-optimization-keras'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tuner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Bayesian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">knapsack_solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CBC_MIP'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regularization_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[1e-06,</span> <span class="pre">1.8478497974222906e-06,</span> <span class="pre">3.414548873833601e-06,</span> <span class="pre">6.30957344480193e-06,</span> <span class="pre">1.165914401179831e-05,</span> <span class="pre">2.1544346900318823e-05,</span> <span class="pre">3.9810717055349695e-05,</span> <span class="pre">7.356422544596421e-05,</span> <span class="pre">0.00013593563908785255,</span> <span class="pre">0.00025118864315095795,</span> <span class="pre">0.00046415888336127773,</span> <span class="pre">0.0008576958985908938,</span> <span class="pre">0.001584893192461114,</span> <span class="pre">0.0029286445646252374,</span> <span class="pre">0.0054116952654646375,</span> <span class="pre">0.01]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hls4ml.optimization.keras.optimize_model" title="Permalink to this definition"></a></dt>
<dd><p>Top-level function for optimizing a Keras model, given objectives</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – Model to be optimized</p></li>
<li><p><strong>model_attributes</strong> (<em>dict</em>) – Layer-wise model attributes,
obtained from hls4ml.optimization.get_attributes_from_keras_model(…)</p></li>
<li><p><strong>objective</strong> (<a class="reference internal" href="hls4ml.optimization.objectives.html#hls4ml.optimization.objectives.ObjectiveEstimator" title="hls4ml.optimization.objectives.ObjectiveEstimator"><em>hls4ml.optimization.objectives.ObjectiveEstimator</em></a>) – Parameter, hardware or user-defined objective of optimization</p></li>
<li><p><strong>scheduler</strong> (<a class="reference internal" href="hls4ml.optimization.html#hls4ml.optimization.scheduler.OptimizationScheduler" title="hls4ml.optimization.scheduler.OptimizationScheduler"><em>hls4ml.optimization.scheduler.OptimizationScheduler</em></a>) – Sparsity scheduler, choose between constant, polynomial and binary</p></li>
<li><p><strong>X_train</strong> (<em>np.array</em>) – Training inputs</p></li>
<li><p><strong>y_train</strong> (<em>np.array</em>) – Training labels</p></li>
<li><p><strong>X_val</strong> (<em>np.array</em>) – Validation inputs</p></li>
<li><p><strong>y_val</strong> (<em>np.array</em>) – Validation labels</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Batch size during training</p></li>
<li><p><strong>epochs</strong> (<em>int</em>) – Maximum number of epochs to fine-tune model, in one iteration of pruning</p></li>
<li><p><strong>optimizer</strong> (<em>keras.optimizers.Optimizer</em><em> or </em><em>equivalent-string description</em>) – Optimizer used during training</p></li>
<li><p><strong>loss_fn</strong> (<em>keras.losses.Loss</em><em> or </em><em>equivalent loss description</em>) – Loss function used during training</p></li>
<li><p><strong>validation_metric</strong> (<em>keras.metrics.Metric</em><em> or </em><em>equivalent loss description</em>) – Validation metric, used as a baseline</p></li>
<li><p><strong>increasing</strong> (<em>boolean</em>) – If the metric improves with increased values;
e.g. accuracy -&gt; increasing = True, MSE -&gt; increasing = False</p></li>
<li><p><strong>rtol</strong> (<em>float</em>) – Relative tolerance;
pruning stops when pruned_validation_metric &lt; (or &gt;) rtol * baseline_validation_metric</p></li>
<li><p><strong>callbacks</strong> (<em>list of keras.callbacks.Callback</em>) – </p></li>
<li><p><strong>ranking_metric</strong> (<em>string</em>) – Metric used for ranking weights and structures;
currently supported l1, l2, saliency and Oracle</p></li>
<li><p><strong>local</strong> (<em>boolean</em>) – Layer-wise or global pruning</p></li>
<li><p><strong>verbose</strong> (<em>boolean</em>) – Display debug logs during model optimization</p></li>
<li><p><strong>rewinding_epochs</strong> (<em>int</em>) – Number of epochs to retrain model without weight freezing,
allows regrowth of previously pruned weights</p></li>
<li><p><strong>cutoff_bad_trials</strong> (<em>int</em>) – After how many bad trials (performance below threshold),
should model pruning / weight sharing stop</p></li>
<li><p><strong>directory</strong> (<em>string</em>) – Directory to store temporary results</p></li>
<li><p><strong>tuner</strong> (<em>str</em>) – Tuning algorithm, choose between Bayesian, Hyperband and None</p></li>
<li><p><strong>knapsack_solver</strong> (<em>str</em>) – Algorithm to solve Knapsack problem when optimizing;
default usually works well; for very large networks, greedy algorithm might be more suitable</p></li>
<li><p><strong>regularization_range</strong> (<em>list</em>) – List of suitable hyperparameters for weight decay</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Optimized model</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hls4ml.optimization.html" class="btn btn-neutral float-left" title="hls4ml.optimization package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hls4ml.optimization.objectives.html" class="btn btn-neutral float-right" title="hls4ml.optimization.objectives package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Fast Machine Learning Lab.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>